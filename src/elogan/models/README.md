# Model Architecture

A generative adversarial network is used to predict moves at the desired ELO level. Within this model architecture, the generator network is used to predict the score of a given move. The generator's prediction is calibrated to the desired ELO level by using a style transfer technique. The generator is trained by competing it against a discriminator network, which aims to distinguish generated moves from real moves, in a zero-sum game. This approach is preferable over traditional deep neural networks because it will not directly attempt to predict specific moves made in similar circumstances in previous games and instead will be able to creatively generate novel moves at the desired ELO level, which is highly beneficial for expert players, chess researchers, and students hoping to learn novel techniques.

## Generator Network

For my generator model, I create a feed-forward neural network which takes as input a vector representing the chess board before a move was taken. The neural network consists of multiply fully-connected layers, each of which has 2048 units and is followed by a ReLU activation function, with the exception of the output layer, which has only one unit and has a linear activation function. The output of the generator is the score of the chess move. Positive values represent moves in favor of white, whereas negative values represent moves in favor of black. For the generator model, I use an Adam optimizer with a learning rate of 0.1 and binary cross-entropy loss with an added term to use the discriminator to regularize the generator, which further constrains the generator to produce outputs closely matching the ELO ranking inputted into the discriminator; a higher value of $k$ causes the generator to more closely match the target ELO level whereas a smaller value of $k$ relaxes this constraint. For the training, I set k = 5.

## Discriminator Network

For my discriminator model, I create a feed-forward neural network which takes as input an ELO rating and a chess move, represented as a tuple of two vectors: the first vector represents the chess board before the move was taken, and the second vector represents the chess board after the move was taken. The neural network consists of multiple fully-connected layers, each of which has 2048 units and is followed by a ReLU activation function, with the exception of the output layer, which has only one unit and which has a sigmoid activation function. For the discriminator model, I use an Adam optimizer with a learning rate of 0.00001 and binary cross-entropy loss.
